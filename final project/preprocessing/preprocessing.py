# -*- coding: utf-8 -*-
"""preprocessing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1P80a73FEm_Hx4x1a8Q5h9BxLZ0gZOnN2

Instructions (will analyze anyone's letterboxd data!)
- Within Letterboxd app, navigate to Settings -> data -> export as csv
- This notebook takes in the diary.csv file that is automatically generated -- just unpack the zip file
- I was only interested in looking at movies I had watched from 2023 onwards, so I filtered early dates out. Adjust for a different timeframe!
- Since color palette analysis is being conducted, need to use API key in order to access each movie poster
"""

from dotenv import load_dotenv
import ones
load_dotenv()

API_KEY = os.getenv("TMDB_API_KEY")

from google.colab import drive
drive.mount('/content/gdrive')

import pandas as pd
df=pd.read_csv('gdrive/MyDrive/diary.csv')

print(df)

pd.to_datetime(df['Watched Date']) # converting to datetime type so can remove earlier entries!

cutoff_datetime = pd.to_datetime('2023-01-01') # removing early than 2023
df_filtered = df[pd.to_datetime(df['Watched Date']) > cutoff_datetime]
print(df_filtered)

print(df_filtered.info())

import matplotlib.pyplot as plt
from PIL import Image
import requests
from io import BytesIO

print(df_filtered['Rating'].value_counts())

"""Adding new columns that I will populate as I go through each movie and find info from each movie's API/metadata"""

df_new = df_filtered.copy()
df_new['Genre'] = None
df_new["Language"]=None
df_new["Runtime"]=None
df_new["Color Palette"]=None
print(df_new)

"""There were one or two entries I couldn't find on TMDB w/my basic loop so I used Chat GPT to help find edge cases:
- it cleans the title first (gets rid of extra characters/normalizes etc.)
- within the loop, it also takes the year into account to make sure that it finds the correct film.
- this is important since some movies might have been published under multiple titles - this was the case with "Raw" (2016) which was originally titled "Grave"   
- finally, if title isn't found, searches the TV show database as well (e.g. "Over the Garden Wall" is under TV!)

Prints out if wasn't able to find movie. from my dataset, the only ones it couldn't find were the Black Mirror episodes, which makes sense because of their format. So, won't be taking them into account for color analysis
"""

import requests, time, unicodedata

def clean_title(t):
    return unicodedata.normalize("NFKD", t).encode("ascii", "ignore").decode()
# cleaning the titles before making the request
for index, row in df_new.iterrows():
    title = clean_title(row['Name'])
    # below, making sure to take year into account when finding in database - helps prevent
    year = int(row.iloc[2]) if not pd.isna(row.iloc[2]) else None  # 3rd coln
    params = {
        "api_key": TMDB_API_KEY,
        "query": title,
        "include_adult": True
    }
    if year:
        params["year"] = year

    resp = requests.get("https://api.themoviedb.org/3/search/movie", params=params).json()

    # try looking for tv show results too if still isn't showing up
    if not resp["results"]:
        resp = requests.get("https://api.themoviedb.org/3/search/tv", params=params).json()

    if resp["results"]:
        movie_id = resp["results"][0]["id"]
        details = requests.get(
            f"https://api.themoviedb.org/3/movie/{movie_id}?api_key={TMDB_API_KEY}"
        ).json()
        result = resp["results"][0]
        title_found = result.get("title") or result.get("name")
        poster_path = result.get("poster_path")

        # also, I want to store some other info for stats. info is: genre, runtime, original language
        df_new.at[index, "Genre"] = [g["id"] for g in details.get("genres", [])] # here, just stored as an ID -> before analysis, will have to translate ID to actual genre (get from TMDB)
        df_new.at[index, "Runtime"] = details.get("runtime")
        df_new.at[index, "Language"] = details.get("original_language")# same here, just an abbreviated version of language

        if poster_path:
          url = f"https://image.tmdb.org/t/p/w500{poster_path}"
          response = requests.get(url)
          img = Image.open(BytesIO(response.content)) # just loading img in from url
          palette = img.convert("P", palette=Image.ADAPTIVE, colors=3)
          palette_colors = palette.getpalette()[:3*3]  # top 3 RGB vals!

          df_new.at[index, "Color Palette"] = palette_colors
          # print("Poster URL:", f"https://image.tmdb.org/t/p/w500{poster_path}") #-> not printing out the path anymore

    # prints out any unsuccessful finds, just to be aware of what they are!
    else:
        print(f"NOT FOUND: {title}")

    time.sleep(0.25)

# genre/language mappings access - define these
genre_map = {g["id"]: g["name"] for g in requests.get(
    f"https://api.themoviedb.org/3/genre/movie/list?api_key={TMDB_API_KEY}"
).json()["genres"]}

langs_resp = requests.get(
    f"https://api.themoviedb.org/3/configuration/languages?api_key={TMDB_API_KEY}"
).json()
lang_map = {l["iso_639_1"]: l["english_name"] for l in langs_resp}


# now create new colns with full genre (rather than just ID), non-abbreviated language
df_new["GenreNames"] = df_new["Genre"].apply(
    lambda ids: [genre_map.get(i, "Unknown") for i in ids] if isinstance(ids, list) else []
)
df_new["LanguageName"] = df_new["Language"].map(lang_map)

print(df_new)

from sklearn.cluster import KMeans
import numpy as np
import pandas as pd

# creating new df that drops any with missing palette entries
df_colors = df_new.dropna(subset=["Color Palette"]).copy()

# palette list -> 2D array
X = np.vstack(df_colors["Color Palette"].values)

# run K-Means clustering:
k = 7   # adjust based on how many clusters you want
kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
df_colors["Palette Cluster"] = kmeans.fit_predict(X)

# print out how the head of df is classified
print(df_colors[["Name", "Color Palette", "Palette Cluster"]].head())

import matplotlib.pyplot as plt


#printing out centers of each cluster


cluster_centers = kmeans.cluster_centers_.reshape(k, 3, 3).astype(int)


plt.figure(figsize=(10, 2))
for i, cluster in enumerate(cluster_centers):
   plt.subplot(1, k, i+1)
   plt.imshow([cluster / 255])
   plt.axis("off")
   plt.title(f"Cluster {i}")
plt.show()
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import numpy as np


# pca dimensionality reduction
pca = PCA(n_components=2, random_state=42)
X_2D = pca.fit_transform(X)


# storing in DataFrame for easy plotting
df_colors["PCA1"] = X_2D[:, 0]
df_colors["PCA2"] = X_2D[:, 1]


# creating basic plot visualization in 2D
plt.figure(figsize=(8, 6))
scatter = plt.scatter(
   df_colors["PCA1"],
   df_colors["PCA2"],
   c=df_colors["Palette Cluster"],
   cmap="tab10",
   s=60,
   alpha=0.8,
   edgecolors="k"
)
plt.title("Movie Color Palette Clusters (PCA-reduced to 2D)")
plt.xlabel("PCA Component 1")
plt.ylabel("PCA Component 2")
plt.grid(True, alpha=0.3)
plt.legend(*scatter.legend_elements(), title="Cluster")
plt.show()

"""## Now, collecting some summary statistics to report back to user:"""

avescore = df_new['Rating'].mean()
print(avescore)

genre_counts = (
    df_new["GenreNames"]
    .explode()             # turns each list element into its own row
    .dropna()
    .value_counts()       #  NOW do value counts

)

print(genre_counts)

top_genres=genre_counts.head(5).index.tolist()

df_new["Watched Date"] = pd.to_datetime(df_new["Watched Date"], errors="coerce")
df_new["Month"] = df_new["Watched Date"].dt.to_period("M")   # e.g. 2025-03
monthly_counts = df_new.groupby("Month").size()
monthly_avg = monthly_counts.mean()
print(monthly_avg)

# now doing the same thing with tags
all_tags = (
    df["Tags"]
    .dropna()
    .astype(str)
    .str.split(",")
    .sum()  # concatenates lists across all rows
)
all_tags = [t.strip() for t in all_tags]
pd.Series(all_tags).value_counts().head(10)

top_tags= pd.Series(all_tags).value_counts().head(5).index.tolist()

unique_languages = df_new["LanguageName"].dropna().unique().tolist()
unique_languages.remove('No Language') # here, removing the No Language Tag
print("Unique Languages:", unique_languages)

runtime= df_new["Runtime"].dropna().sum() / 60
print(runtime)

summary_stats ={
    "Average Rating": avescore,
    "Top 5 Genres": top_genres,
    "Top 5 Tags": top_tags,
    "Languages Watched in": unique_languages,
    "Monthly Average": monthly_avg,
    "Total Movie Number": len(df_new),
    "Total Runtime (hrs)": runtime
}
pd.DataFrame([summary_stats]).to_csv("summary_stats.csv", index=False)

"""## Finally, exporting the following csv files:
- Summary stats
- diary_final (all info, removing some uneccessary columns from df_new)
- df_colors (has cluster classifications!)
"""

diary_final = df_new.drop(['Rewatch', 'Genre', 'Language'], axis=1)
print(diary_final)

diary_final.to_csv("diary_final.csv")
df_colors.to_csv("df_colors.csv")

from google.colab import files
files.download("summary_stats.csv")
files.download("diary_final.csv")
files.download("df_colors.csv")