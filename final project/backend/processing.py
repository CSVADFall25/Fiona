# -*- coding: utf-8 -*-
"""preprocessing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1P80a73FEm_Hx4x1a8Q5h9BxLZ0gZOnN2

- Since color palette analysis is being conducted, need to use API key in order to access each movie poster
"""

# importing all necessary libraries here instead of sporadically thru out lol
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from PIL import Image
import requests
from io import BytesIO
import requests, time, unicodedata

# installing and importing scrapxd library :P
!pip install scrapxd
from scrapxd.client import Scrapxd
# instantiating client 
client = Scrapxd()


# basic get diary webscrape 
def get_diary(username, api_key):
    my_user = client.get_user("username")
    diary = my_user.diary  # ideally, should scrape diary ... 
    return(diary)



def clean_title(t):
    return unicodedata.normalize("NFKD", t).encode("ascii", "ignore").decode()


# runs k means and get full clean df with color groupings
def run_kmeans(diary, api_key): # need to make sure the structure of the diary is the same - e.g. same labels for colns
    # currently just taking it to be true. will troubleshoot when inevitably isn't 
    # converting to datetime type so can remove earlier entries!
    cutoff_datetime = pd.to_datetime('2023-01-01') # removing early than 2023
    df_filtered = df[pd.to_datetime(diary['Watched Date']) > cutoff_datetime]

    df_new = df_filtered.copy()
    # Adding new columns that I will populate as I go through each movie and find info from each movie's API/metadata
    df_new['Genre'] = None
    df_new["Language"]=None
    df_new["Runtime"]=None
    df_new["Color Palette"]=None


    # massive for loop
    for index, row in df_new.iterrows():
        title = clean_title(row['Name']) # cleaning the titles before making the request
    # below, making sure to take year into account when finding in database
        year = int(row.iloc[2]) if not pd.isna(row.iloc[2]) else None  # 3rd coln
        params = {
            "api_key": api_key,
            "query": title,
            "include_adult": True
        }
        if year:
            params["year"] = year

        resp = requests.get("https://api.themoviedb.org/3/search/movie", params=params).json()

        # try looking for tv show results too if still isn't showing up
        if not resp["results"]:
            resp = requests.get("https://api.themoviedb.org/3/search/tv", params=params).json()

        if resp["results"]:
            movie_id = resp["results"][0]["id"]
            details = requests.get(
                f"https://api.themoviedb.org/3/movie/{movie_id}?api_key={api_key}"
            ).json()
            result = resp["results"][0]
            title_found = result.get("title") or result.get("name")
            poster_path = result.get("poster_path")

         # also, I want to store some other info for stats. info is: genre, runtime, original language
            df_new.at[index, "Genre"] = [g["id"] for g in details.get("genres", [])] # here, just stored as an ID -> before analysis, will have to translate ID to actual genre (get from TMDB)
            df_new.at[index, "Runtime"] = details.get("runtime")
            df_new.at[index, "Language"] = details.get("original_language")# same here, just an abbreviated version of language

            if poster_path:
                url = f"https://image.tmdb.org/t/p/w500{poster_path}"
                response = requests.get(url)
                img = Image.open(BytesIO(response.content)) # just loading img in from url
            palette = img.convert("P", palette=Image.ADAPTIVE, colors=3)
            palette_colors = palette.getpalette()[:3*3]  # top 3 RGB vals!

            df_new.at[index, "Color Palette"] = palette_colors
          # print("Poster URL:", f"https://image.tmdb.org/t/p/w500{poster_path}") #-> not printing out the path anymore
        time.sleep(0.25)

    # genre/language mappings access - define these
    genre_map = {g["id"]: g["name"] for g in requests.get(
    f"https://api.themoviedb.org/3/genre/movie/list?api_key={API_KEY}"
    ).json()["genres"]}

    langs_resp = requests.get(
        f"https://api.themoviedb.org/3/configuration/languages?api_key={API_KEY}"
    ).json()
    lang_map = {l["iso_639_1"]: l["english_name"] for l in langs_resp}


    # now create new colns with full genre (rather than just ID), non-abbreviated language
    df_new["GenreNames"] = df_new["Genre"].apply(
    lambda ids: [genre_map.get(i, "Unknown") for i in ids] if isinstance(ids, list) else []
    )
    df_new["LanguageName"] = df_new["Language"].map(lang_map)

    # creating new df that drops any with missing palette entries
    df_colors = df_new.dropna(subset=["Color Palette"]).copy()

    # palette list -> 2D array
    X = np.vstack(df_colors["Color Palette"].values)


    # run K-Means clustering:
    k = 7   # adjust based on how many clusters you want
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    df_colors["Palette Cluster"] = kmeans.fit_predict(X)

    # centers of each cluster
    cluster_centers = kmeans.cluster_centers_.reshape(k, 3, 3).astype(int)

    # pca dimensionality reduction
    pca = PCA(n_components=2, random_state=42)
    X_2D = pca.fit_transform(X)

    # storing in DataFrame for easy plotting
    df_colors["PCA1"] = X_2D[:, 0]
    df_colors["PCA2"] = X_2D[:, 1]
    return (df_colors)



# computes summary stats
def compute_summary(diary_clean): 
    avescore = diary_clean['Rating'].mean()
    genre_counts = (
    diary_clean["GenreNames"]
    .explode()  # turns each list element into its own row
    .dropna()
    .value_counts() #  NOW do value counts
   )

   top_genres=genre_counts.head(5).index.tolist()
   diary_clean["Watched Date"] = pd.to_datetime(diary_clean["Watched Date"], errors="coerce")
   diary_clean["Month"] = diary_clean["Watched Date"].dt.to_period("M")   # e.g. 2025-03
   monthly_counts = diary_clean.groupby("Month").size()
   monthly_avg = monthly_counts.mean()

   # now doing the same thing with tags
   all_tags = (
     df["Tags"]
     .dropna()
     .astype(str)
     .str.split(",")
     .sum()  # concatenates lists across all rows
   ) 
   all_tags = [t.strip() for t in all_tags]
   pd.Series(all_tags).value_counts().head(10)

   top_tags= pd.Series(all_tags).value_counts().head(5).index.tolist()
   unique_languages = diary_clean["LanguageName"].dropna().unique().tolist()
   unique_languages.remove('No Language') # here, removing the No Language Tag
   runtime= diary_clean["Runtime"].dropna().sum() / 60

   summary_stats ={
    "Average Rating": avescore,
    "Top 5 Genres": top_genres,
    "Top 5 Tags": top_tags,
    "Languages Watched in": unique_languages,
    "Monthly Average": monthly_avg,
    "Total Movie Number": len(diary_clean),
    "Total Runtime (hrs)": runtime
   }
   return summary_stats



def full_bundle(username, key):
    df = get_diary(username, key)
    df_colors = run_kmeans(df, key) # just takes in the cleaned color dataframe 
    summary = compute_summary(df) # also need key here since need to search TMDB

    return{
        "summary":summary.to_dict(), # here, is "summary" and "df_colors" just the return call at the end - check this 
        "df full": df_colors.to_dict() # here, need to convert the dataframe to be a dictionary for export 
    }